{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices: Advanced Regression Techniques\n",
    "\n",
    "This project aims to predict the house price based on various features.\n",
    "\n",
    "#### Dataset link\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The lifecycle of the project\n",
    "1. Data Analysis\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Model Building\n",
    "5. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Phase\n",
    "#### To understand more about:\n",
    "1. Missing values\n",
    "2. All the numerical variables and its distribution.\n",
    "3. Categorical variables and its cardinality\n",
    "4. Outliers and abnormalities\n",
    "5. Relationship between independent and dependent feature (SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# For features engineering\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# for feature slection\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Display all the columns of the dataframe\n",
    "pd.pandas.set_option('display.max_columns',None)\n",
    "pd.pandas.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df1=pd.read_csv('train.csv')\n",
    "#df1=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the data\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the first 5 rows of the dataset\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The statistical summary of dataset\n",
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning the dtypes of columns' and how many non-null \n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Features have missing values\n",
    "cols_na=df1.columns[df1.isnull().any()]\n",
    "# The percentage of missing values for eah feature\n",
    "print(\"Missing Values by Column\")\n",
    "print(\"-\"*30)\n",
    "for col in cols_na:\n",
    "    print(col,df1[col].count(),df1[col].isnull().sum(),np.round(df1[col].isnull().mean()*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the numerical features that have missing values (more than half) and SalePrice before drop them, by plotting diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df1[cols_na])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Correlation between the numerical features and the target variable using jointplot visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_na=[col for col in df1.columns[df1.isnull().any()] if df1[col].dtype!='O']\n",
    "\n",
    "for col in num_cols_na:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.jointplot(x=df1[col], y=df1[\"SalePrice\"], kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the categorical features that have missing values (more than half) and SalePrice before drop them, by plotting diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from df1 has categorical features that ave null values\n",
    "df_na=df1.copy()\n",
    "cat_cols_na=[col for col in df_na.columns[df_na.isnull().any()] if df_na[col].dtype=='O']\n",
    "df_na=df_na[cat_cols_na+ ['SalePrice']]\n",
    "for col in cat_cols_na:\n",
    "    # Indicate 1 if the observation is null or 0 otherwise.\n",
    "    df_na[col]= np.where(df_na[col].isnull(),1,0)\n",
    "    # Plot the mean of SalePrice feature for each feature\n",
    "    df_na.groupby(col)['SalePrice'].mean().plot.bar()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Mean House Price')\n",
    "    plt.title(col)\n",
    "    plt.show()\n",
    "df_na.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between the missing values and the dependent variable (SalePrice) is clearly visible.So We can't just drop these features. In the next Phase (Feature Engineering), will solve this proplem by replacing the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols is a list of numerical features\n",
    "num_cols=df1.select_dtypes(exclude='object').columns\n",
    "# num_cols= [col for col in df1.columns if df1[col].dtype!='O']\n",
    "df1[num_cols].shape\n",
    "df1[num_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal variables\n",
    "From data_description.txt file, there are 4 date type variables (YearBuilt, YearRemodAdd, GarageYrBlt, YrSold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    print(col, df1[col].dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_cols is a list of temporal features\n",
    "temp_cols = [col for col in num_cols if 'Yr' in col or 'Year' in col]\n",
    "# data type of the temporal features in the dataset\n",
    "for col in temp_cols:\n",
    "    print(col, df1[col].dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset, the data type of the 4 temporal features is int or float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the temporal features and SalePrice before drop them, by plotting diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in temp_cols:\n",
    "    df1.groupby(col)['SalePrice'].mean().plot()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Mean House Price')\n",
    "    plt.title('Mean House Price vs. '+col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the houses' ages and SalePrice, by plotting scatter diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from df1\n",
    "df_temp=df1.copy()\n",
    "\n",
    "\n",
    "for col in temp_cols[:-1]:\n",
    "        # Calculate the house age related to the other temporal variables\n",
    "        df_temp[col]=df_temp['YrSold']-df_temp[col]\n",
    "        # Plot the relationship between the houses' ages (related to the other temporal variables) and SalePrice\n",
    "        plt.scatter(df_temp[col],df_temp['SalePrice'])\n",
    "        plt.xlabel('House age since '+col)\n",
    "        plt.ylabel('Sale Price')\n",
    "        plt.title('House Price vs. house age since '+col)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relation between the temporal features and the dependent variable (SalePrice) is clearly visible. There is an inverse relationship between the age of the house and its price. In the next Phase (Feature Engineering), will solve this proplem by changing these features' type to date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continous and Discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_cols= [col for col in num_cols if len(df1[col].unique())<25 and col not in temp_cols]\n",
    "con_cols= [col for col in num_cols if col not in dis_cols and col not in temp_cols+['Id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[dis_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[con_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the continous and discrete variables and SalePrice, by plotting bar diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the discrete values by creating barcharts\n",
    "for col in dis_cols:\n",
    "    df1.groupby(col)['SalePrice'].mean().plot.bar()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Mean House Price')\n",
    "    plt.title('Mean House Price vs. '+ col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear relationship between the discrete numbers and SalePrice, such as Exponential relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analyse the continuous values by creating histograms to understand the distribution\n",
    "for col in con_cols:\n",
    "    df1[col].hist(bins=25)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the continuous numbers and SalePrice for most of the charts are not Normal distribution. In the next cell, will solve this proplem by changing these features to Normal distribution by using logarithmic transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logarithmic transformation to changing continuous features to Normal distribution\n",
    "for col in con_cols:\n",
    "    df_con=df1.copy()\n",
    "    if 0 in df_con[col].unique() or col in ['SalePrice']:\n",
    "    #if 0 in df_con[col].unique():\n",
    "        pass\n",
    "    else:\n",
    "        df_con[col]=np.log(df_con[col])\n",
    "        df_con['SalePrice']=np.log(df_con['SalePrice'])\n",
    "        plt.scatter(df_con[col],df_con['SalePrice'])\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(col)\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a monotonic relationship between GrLivArea, 1stFlrSF and SalePrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols=[col for col in df1.columns if df1[col].dtypes=='O']\n",
    "cat_cols= df1.select_dtypes(include='object').columns\n",
    "df1[cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of categories for each categorical features\n",
    "for col in cat_cols:\n",
    "    print(col,len(df1[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the relationship between the categorical variables and SalePrice, by plotting bar diagrams for these relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    df1.groupby(col)['SalePrice'].mean().plot.bar()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Mean House Price')\n",
    "    plt.title('Mean House Price vs. '+ col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using boxplot to show the outliers for the continuous variables\n",
    "for col in con_cols:\n",
    "    df_con=df1.copy()\n",
    "    if 0 in df_con[col].unique():\n",
    "        pass\n",
    "    else:\n",
    "        df_con[col]=np.log(df_con[col])\n",
    "        df_con.boxplot(column=col)\n",
    "        plt.ylabel(col)\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Phase\n",
    "#### Performing:\n",
    "1. Missing values.\n",
    "2. Temporal variables.\n",
    "3. Categorical variables.\n",
    "4. Standardise the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid data leakage, should split dataset to train and test datasets then run the feature engineering on both. But our data is already splitted to train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_test_split modelin sklearn package\n",
    "#X_train,X_test,y_train,y_test=train_test_split(df1,df1['SalePrice'],test_size=0.1,random_state=0)\n",
    "#X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Missing values.\n",
    "Drop the columns that have a lot of missing data and fill the rest with appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy from dataset\n",
    "df2=df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace the null values in any dataframe\n",
    "def replaceNull(df,colList,type):\n",
    "    if type=='categorical':\n",
    "        new_df=df.copy()\n",
    "        new_df[colList]=new_df[colList].fillna('n/a')    \n",
    "        return new_df\n",
    "    if type=='numerical':\n",
    "        new_df=df.copy()\n",
    "        for col in colList:\n",
    "            new_df[col]=new_df[col].fillna(new_df[col].median())    \n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Replace the missing values in the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of missing values in the categorical features.\n",
    "cat_na=[col for col in df2[cat_cols] if df2[col].isnull().sum()>0]\n",
    "for col in cat_na:\n",
    "    print(col, np.round(df2[col].isnull().mean()*100,2), '% missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alley, PoolQC, Fence, and MiscFeature features have more than 75% of their values are null. So, will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(columns= {'Alley', 'PoolQC', 'Fence', 'MiscFeature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cat_cols=[col for col in df2.columns if df2[col].dtype=='O']\n",
    "new_cat_cols= df2.select_dtypes(include='object').columns\n",
    "new_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the rest of categorical missing values with (n/a)\n",
    "df3= replaceNull(df2,new_cat_cols,'categorical')\n",
    "for col in new_cat_cols:\n",
    "    print(col, df3[col].isnull().sum()>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replace the missing values in the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of missing values in the numerical features.\n",
    "num_na=[col for col in df3[num_cols] if df3[col].isnull().sum()>0]\n",
    "for col in num_na:\n",
    "    print(col, np.round(df3[col].isnull().mean()*100,2), '% missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just three features have null values and all of them have low percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have outliers, prefere to replace the null values with median or mode, not with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values in the numerical features with median\n",
    "df4= replaceNull(df3,num_na,'numerical')\n",
    "for col in num_na:\n",
    "    print(col, df4[col].isnull().sum()>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Temporal variables.\n",
    "Change the data values of the temporal variables from year to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df4.copy()\n",
    "for col in temp_cols:\n",
    "    if col!='YrSold':\n",
    "        df5[col]=df5['YrSold']-df5[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5[temp_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Numerical variables.\n",
    "Change the numerical variables from skewed distribution to log normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list that has all numeric independent features\n",
    "col_num=[col for col in df5.columns if col not in ['Id','SalePrice'] and df5[col].dtype!='O']\n",
    "#col_num=[col for col in df5.columns if col not in ['Id'] and df5[col].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot diagnostic function that is ploting histogram and Q-Q plot, to detect the skewed distributed features.\n",
    "def diagnostic_plt(df,col):\n",
    "    plt.figure(figsize=(15,6))\n",
    "    # Histogram plot\n",
    "    plt.subplot(1,2,1)\n",
    "    df[col].hist()\n",
    "    # Q-Q plot\n",
    "    plt.subplot(1,2,2)\n",
    "    stats.probplot(df[col],dist='norm',plot=plt)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and Q-Q plots for each numerical feature in the dataset\n",
    "for col in col_num:\n",
    "    diagnostic_plt(df5,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs, most of the numerical features have a skewed distribution. In the Linear Regression, transforming data to better fit the Gaussian Distribution. In this case, Logarithmic transformation will apply on the skewed distributed features to convert it to normal distributed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.copy()\n",
    "skewed_cols=['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea','MiscVal', 'SalePrice']\n",
    "#skewed_cols=['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea','MiscVal']\n",
    "for col in skewed_cols:\n",
    "    df6[col]=np.log(df6[col]+1)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Combine levels.\n",
    "Change all rare categorical variables (which have frequency less than 1% of the dataset) in the whole dataset to a same value/label ('rare_val'), using their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols= [col for col in df5.columns if df5[col].dtype=='O']\n",
    "cat_cols= df5.select_dtypes(include='object').columns\n",
    "df5[cat_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=df6.copy()\n",
    "for col in cat_cols:\n",
    "    temp=df7.groupby(col)['SalePrice'].count()/len(df7)\n",
    "    #temp=df7.groupby(col)['Id'].count()/len(df7)\n",
    "    temp_df=temp[temp>0.01].index\n",
    "    df7[col]=np.where(df7[col].isin(temp_df),df7[col],'rare_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the percentage of cardinality in each categorical feature after compined the rera values in one vaue\n",
    "for col in cat_cols:\n",
    "    temp=np.round((df7.groupby(col)['SalePrice'].count()/len(df7))*100,2)\n",
    "    #temp=np.round((df7.groupby(col)['Id'].count()/len(df7))*100,2)\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Convert to Number.\n",
    "Some ML libraries do not take categorical variables as input. Thus, we convert them into numerical variables using label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8=df7.copy()\n",
    "for col in cat_cols:\n",
    "        label_encoder=df8[col].value_counts().index\n",
    "        #label_encoder=df8.groupby([col])['Id'].mean().sort_values().index\n",
    "        label_encoder={j:i for i,j in enumerate(label_encoder)}\n",
    "        df8[col]= df8[col].map(label_encoder)\n",
    "\n",
    "# Check if the dataset has any categorical values\n",
    "# cat_cols= [col for col in df8.columns if df8[col].dtype=='O']\n",
    "cat_cols= df8.select_dtypes(include='object').columns\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Features scaling.\n",
    "This dataset has many features computed by different measurements and units, so it is necessary to scale the features (except Id and SalePrice columns) to apply the ML models.\n",
    "Two common features scaling (normalizations) are:\n",
    "1. Z-score\n",
    "2. MinMax\n",
    "\n",
    "In this project, I use MinMax normalization in order to handle the outliers better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9=df8.copy()\n",
    "# Make a list has all independent features\n",
    "scale_cols=[col for col in df9.columns if col not in ['Id','SalePrice']]\n",
    "#scale_cols=[col for col in df9.columns if col not in ['Id']]\n",
    "# Use MinMaxScaler from sklearn.preprocessing to scale the features\n",
    "scaler=MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "scaler.fit(df9[scale_cols])\n",
    "scaled=scaler.transform(df9[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Id and SalePrice variables to the ttransformed dataset\n",
    "df10=pd.concat([df9[['Id', 'SalePrice']].reset_index(drop=True), pd.DataFrame(scaled,columns=scale_cols)], axis=1)\n",
    "#df10=pd.concat([df9[['Id']].reset_index(drop=True), pd.DataFrame(scaled,columns=scale_cols)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df10.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection\n",
    "#### To reduce the features for the linear regression and  skip the not useful ones.\n",
    "1. Capture the dependent feature from the dataset (y-train).\n",
    "2. Capture the independent feature from the dataset (X-train).\n",
    "3. Select the unuseful independent feature from X-train and drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Capture the dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df10[['SalePrice']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Capture the independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df10.drop(['Id','SalePrice'],axis=1)\n",
    "#X_test=df10.drop(['Id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- select the unuseful independent feature\n",
    "To select the useless independent features will use Lasso Regression model and selectFromModel object, which will select the features with non-zero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select alpha=0.5% (equivalent of penalty).Bigger than this will select less features.\n",
    "sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0))\n",
    "sel_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results, which True means that this feature is important to the Regression algorithm and false means not.\n",
    "for col in X_train.columns:\n",
    "    print(col,sel_model.get_support()[X_train.columns.get_loc(col)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of selected features\n",
    "selected_cols=X_train.columns[(sel_model.get_support())]\n",
    "print('Total features: {}'.format((X_train.shape[1])))\n",
    "print('No. features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_model.estimator_.coef_ == 0)))\n",
    "print('No. selected features is {} features:'.format(len(selected_cols)))\n",
    "list(X_train[selected_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unselected independent features\n",
    "X_train=X_train[selected_cols]\n",
    "\n",
    "#X_test=X_test[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and transformed dataset.\n",
    "X_train.to_csv('X_train.csv',index=False)\n",
    "y_train.to_csv('y_train.csv',index=False)\n",
    "#X_test.to_csv('X_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building Phase\n",
    "1. Split the data\n",
    "2. Defining evaluation functions.\n",
    "3. Machine Learning Models.\n",
    "4. Model Comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split the train data as x_train , x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib\n",
    "X_train=pd.read_csv('X_train.csv')\n",
    "y_train=pd.read_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "def best_model(X_train, y_train, X_test, y_test):\n",
    "    algos={\n",
    "#         'linear_regression':{\n",
    "#             'model':LinearRegression(),\n",
    "#             'params':{\n",
    "#                 'normalize':[True,False]\n",
    "#             }\n",
    "#         },\n",
    "#         'lasso':{\n",
    "#             'model':Lasso(),\n",
    "#             'params':{\n",
    "#                 'alpha':[1,2],\n",
    "#                 'selection':['random','cycle']\n",
    "#             }\n",
    "#         },\n",
    "#         'ridge_regression':{\n",
    "#             'model':Ridge(),\n",
    "#             'params':{}\n",
    "#         },\n",
    "#         'decision_tree':{\n",
    "#             'model':DecisionTreeRegressor(),\n",
    "#             'params':{\n",
    "#                 'criterion':['mse','friedman_mse'],\n",
    "#                 'splitter':['best','random'],\n",
    "#                 'max_depth' : [10],\n",
    "#                 'min_samples_leaf' : [2]\n",
    "#             }\n",
    "#         },\n",
    "#         'elastic_net':{\n",
    "#             'model':ElasticNet(),\n",
    "#             'params':{}\n",
    "#         },\n",
    "#         'SVR':{\n",
    "#             'model':SVR(),\n",
    "#             'params':{\n",
    "#                 'C':[100000,0.7],\n",
    "#                 'kernel' : ['rbf'],\n",
    "#                 'gamma' : ['auto'],\n",
    "#                 'degree':[4],\n",
    "# #                 'epsilon':[0.002],\n",
    "#                 'coef0':[20]\n",
    "#             }\n",
    "#         },\n",
    "#         'Random_Forest_Regressor':{\n",
    "#             'model':RandomForestRegressor(),\n",
    "#             'params':{\n",
    "#                 'n_estimators':[100,1500],\n",
    "#                 'max_depth' : [3]\n",
    "#             }\n",
    "#         },\n",
    "        'GradientBoostingRegressor':{\n",
    "            'model':GradientBoostingRegressor(),\n",
    "            'params':{\n",
    "               #                 'n_estimators':[100,1500],\n",
    "                'learning_rate':[1, 0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "#                 'learning_rate':[.01],\n",
    "#                 'max_depth':[3],\n",
    "#                 'min_samples_leaf':[1],\n",
    "                'max_depth':range(1,32,2), \n",
    "                'min_samples_split':range(200,1001,200),\n",
    "                'n_estimators':range(20,1500,10),\n",
    "                'min_samples_leaf':range(30,71,10),\n",
    "#                 'max_features':range(0,5),\n",
    "                'subsample':[.9]\n",
    "            }\n",
    "        }\n",
    "#         ,\n",
    "#         'XGBoost Regressor':{\n",
    "#             'model':XGBRegressor(),\n",
    "#             'params':{\n",
    "#                 'n_estimators':[100],\n",
    "#                 'learning_rate':[0.01]\n",
    "#             }\n",
    "#         }\n",
    "#         ,\n",
    "#         'Polynomial_Regression_d2 ':{\n",
    "#             'model':PolynomialFeatures(),\n",
    "#             'params':{\n",
    "#                 'degree':[2]\n",
    "#             }\n",
    "#         }\n",
    "    }\n",
    "    \n",
    "    ml_models,model_scores,predictions=[],[],[]\n",
    "    cv=ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)\n",
    "    for algo_name,config in algos.items():\n",
    "        gs=GridSearchCV(config['model'],config['params'],cv=cv,return_train_score=False).fit(X_train, y_train)\n",
    "        y_pred=gs.predict(X_test)\n",
    "        # Error validation functions\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_squared = r2_score(y_test, y_pred)\n",
    "        rmse_cv = np.sqrt(-cross_val_score(gs, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5)).mean()\n",
    "        model_scores.append({\n",
    "                          \"Model_name\": algo_name,\n",
    "                          \"best_score\":gs.best_score_,\n",
    "                          \"mean_absolute_error\": mae,\n",
    "                          \"mean_squared_error\": mse,\n",
    "                          \"root_mean_squared_error\": rmse,\n",
    "                          \"r2_score\": r_squared,\n",
    "                          \"RMSE_Cross_Validation\": rmse_cv,\n",
    "                          \"best_params\":gs.best_params_,\n",
    "                            \"best_estimator\":gs.best_estimator_})\n",
    "        predictions.append({\"Model_name\": algo_name,\"y_pred\":y_pred})\n",
    "        print(f'{algo_name} is done')        \n",
    "    return pd.DataFrame(model_scores, columns=[\"Model_name\",\"best_score\",\n",
    "                                            \"mean_absolute_error\",\n",
    "                                            \"mean_squared_error\", \n",
    "                                            \"root_mean_squared_error\",\n",
    "                                            \"r2_score\",\n",
    "                                            \"RMSE_Cross_Validation\",\n",
    "                                            \"best_params\",\n",
    "                                            \"best_estimator\"]\n",
    "                       ).sort_values(by=\"RMSE_Cross_Validation\"\n",
    "                                    ),pd.DataFrame(predictions, columns=[\"Model_name\",\"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores,predictions=best_model(X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(predictions)):\n",
    "    plt.scatter(y_test, predictions['y_pred'][i])\n",
    "    plt.xlabel(\"Reality Prices\")\n",
    "    plt.ylabel(\"Predicted prices\")\n",
    "    plt.title(predictions['Model_name'][i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(ML_models_results[\"Model_name\"], ML_models_results[\"r2_score\"], width = 0.35 , label='r2_score')\n",
    "ax.bar(ML_models_results[\"Model_name\"], ML_models_results[\"RMSE_Cross_Validation\"],width = 0.35 , label='RMSE_Cross_Validation')\n",
    "ax.set_ylabel('Scores')\n",
    "plt.title(\"Evaluation of Models Based on RMSE (Cross-Validated)\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file\n",
    "Prepare test dataset (Cleaning, features engineering and selection )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1=pd.read_csv('test.csv')\n",
    "sample_sub_df1 = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_cols=[col for col in test_df1.columns if test_df1[col].dtypes=='O']\n",
    "num_test_cols=[col for col in test_df1.columns if test_df1[col].dtypes!='O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2=test_df1.copy()\n",
    "test_df2=test_df2.drop(columns= {'Alley', 'PoolQC', 'Fence', 'MiscFeature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_test_cols=[col for col in test_df2.columns if test_df2[col].dtype=='O']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the rest of categorical missing values with (n/a)\n",
    "test_df3= replaceNull(test_df2,new_cat_test_cols,'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the percentage of missing values in the numerical features.\n",
    "num_test_na=[col for col in test_df3[num_test_cols] if test_df3[col].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values in the numerical features with median\n",
    "test_df4= replaceNull(test_df3,num_test_na,'numerical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_cols = [col for col in num_test_cols if 'Yr' in col or 'Year' in col]\n",
    "test_df5=test_df4.copy()\n",
    "for col in temp_test_cols:\n",
    "    if col!='YrSold':\n",
    "        test_df5[col]=test_df5['YrSold']-test_df5[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list that has all numeric independent features\n",
    "test_col_num=[col for col in test_df5.columns if col not in ['Id'] and test_df5[col].dtype!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df6=test_df5.copy()\n",
    "skewed_test_cols=['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea','MiscVal']\n",
    "for col in skewed_test_cols:\n",
    "    test_df6[col]=np.log(test_df6[col]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_cols= [col for col in test_df6.columns if test_df6[col].dtype=='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df7=test_df6.copy()\n",
    "for col in cat_test_cols:\n",
    "        label_encoder=test_df7.groupby([col])['Id'].mean().sort_values().index\n",
    "        label_encoder={j:i for i,j in enumerate(label_encoder)}\n",
    "        test_df7[col]= test_df7[col].map(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df8=test_df7.copy()\n",
    "# Use MinMaxScaler from sklearn.preprocessing to scale the features\n",
    "scaler=MinMaxScaler(copy=True, feature_range=(0, 1)).fit(test_df8)\n",
    "scaled=scaler.transform(test_df8)\n",
    "X_test=pd.DataFrame(scaled,columns=test_df8.columns)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test[selected_cols]\n",
    "X_test.to_csv('X_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction X_test dataset \n",
    "Use GradientBoostRegressor with the best params to predict the sale price for X_test dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores['best_params'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_price = GradientBoostingRegressor(n_estimators=1500,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=3,\n",
    "                min_samples_leaf=1,\n",
    "                random_state=2,\n",
    "                subsample = 0.2).fit(X_train,y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_sub_df1['SalePrice_new'] = np.exp(prediction_price)\n",
    "# sample_sub_df1.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
